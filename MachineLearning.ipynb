{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SashaR39/data/blob/master/MachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Learning\n",
        "You will create a machine learning model that is trained on the MNIST data set. [The MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) is a classic benchmark dataset in the field of machine learning and computer vision. It consists of a set of 70,000 handwritten digit images, each of which is a grayscale image of size 28x28 pixels.\n",
        "\n",
        "The dataset is split into two subsets: a training set of 60,000 images and a test set of 10,000 images. The training set is used to train a machine learning model, while the test set is used to evaluate the performance of the model on new, unseen data.\n",
        "\n",
        "The digits in the dataset were collected from a variety of sources, including high school students and employees of the United States Census Bureau.\n",
        "\n",
        "The MNIST dataset has been used extensively to evaluate the performance of machine learning models for image classification tasks. Many popular machine learning frameworks, including TensorFlow and PyTorch, provide built-in support for loading and working with the MNIST dataset."
      ],
      "metadata": {
        "id": "CiEq1WRsvQCa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QaNg9-0vvPf9",
        "outputId": "b8cfd613-8f17-438d-a42a-2677331081e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2608 - accuracy: 0.9259\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1136 - accuracy: 0.9669\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0985 - accuracy: 0.9709\n",
            "Test accuracy: 0.9708999991416931\n",
            "313/313 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5a8b9d2015ee>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'red'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     plt.xlabel(\"{} ({})\".format(class_names[predicted_label], \n\u001b[0m\u001b[1;32m     52\u001b[0m                                 class_names[true_label]),\n\u001b[1;32m     53\u001b[0m                                 color=color)\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFYAAABWCAYAAABVVmH3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAJ30lEQVR4nO2d3W/aVh+AH2NsbMCYD0O+IEuaDynb0inKXbWpt9uup/1/2/VuJ01adzWprXazbF2jdsraLoSvQPg2AWPs96Kvrb172zVp6wQ6HglxgYHjh8Pv+PzO74Dguq7LnLdO6Lob8K4yFxsQc7EBMRcbEHOxATEXGxBzsQExFxsQ4Ysc5DgO5XIZTdMQBCHoNk0truvS6/VYXl4mFHpFn3QvQLFYdIH57b+3YrH4SmcX6rGapgFQLBZJJBIXeco7SbfbpVAo+D7+iQuJ9b7+iUTiXy3W4yLhcD54BcRcbEDMxQbEXGxAzMUGxFxsQMzFBsRcbEDMxQbEXGxAzMUGxFxsQFwoCfO6uK5Lu93GNE36/T6NRgPHcZAkCVEUEQQBQRBQFIVMJoMkSf5jkiQRiUSCbF6gBCrWcRyKxSJ//vknz54946effsK2bRKJBJFIBFEUCYfDGIbB/v4+uq4Tj8dRFIV4PE4mk3l1QnlKCVxsp9OhUqlQqVSoVqvYto1pmr7YUCjEaDQil8vRbrfRNA1VVdE0jdFohCiKb9wOQRAIhUKEQiFUVfW/FW/jtV9GoGJt2+bnn3/m22+/pdVqcXx8zGQyIRwO+ycrCAKSJPHjjz8iyzKqqiLLMrqus7i4+FZ6rCzLxONxotEo+/v75PN5kskk2Ww2sKWmwHvs6ekpR0dHmKZJs9nEtm3/ZF5UjxeJRP5H7NvoVYqikEwmSSQSLC0toWkakUgE13VnU2w4HObDDz/k888/p9vtUqvVAIjFYsiyzPn5OYPBAMuy6HQ6jMdjut0u5+fnjMdjqtUqoiiiKAqiKGJZFqPR6KXv5w2GkUgEQRCwLAvbthFFkVqthqZprKys4DgOjuOwsrISWAwPXOzNmzdJp9O0221KpRKCILC0tEQsFqPRaNBoNOj3+xwfH2OaJkdHR9RqNQaDAaenp4TDYVKpFJFIhG63S6fTeeF7CYLgx81EIoEoivT7ffr9vn9MLBZjaWkJ27bRNI29vb3gzj2wV+b5yXqjeyQS8WOqYRgoioIsy0SjUQaDgX8fi8U4Ozuj1+vRarUQBIFcLoeiKLRaLdrt9gtDiDc4SZJEMplEFEX++OMPnj59iuu62LaN67qEQiFkWSYcDvTUgxUbCoVYXFzEMAwcx2E8HiMIgj94TSYTJpOJ/5jjOH4oaLValMtlwuEw+XweTdOoVqtUq9UXivUu3WRZJpfLIYoiX3/9NfV6HcuymEwmhEIhdF0nm80Sj8cDrZEI9mPj+Ygsy/KFj0+lUoxGI1KplP/cQqFALBZDVVVUVX2pWG+CkclkEAQBXdf98BAOh/14HYvFkCRptsVelr9eY6qq6ocTSZJYWFh46Zq+d/lmWRa1Wo1er8fp6SmmaSJJEoZhkMlkeP/999nb2yObzQY6+Zg6sV7sk2X5/yTquo6u6//4/F6v5w+AnU6H0WiEJEnouk46nSafz7O+vo6iKP8usa/LZDLBtm3a7Ta//fYbT548oVwu47ousViMnZ0dP95HIpHZHryukvF4jGmaVKtVvv/+e3755Rf/CiKbzXL79m3y+Tyrq6tEo9HAi/tmM8PxAizLot1u02q16Ha7dLtdLMvyr0I0TUPTNGRZvpKKyXemx5bLZe7du0exWKRSqWCaJqFQiFgshq7r5HI5stksiqJcSXveiR7rui79fp9SqUSlUvGnyd7llaIoRKNRotFooBmtvzLTPdZxHFqtFqZp8ujRI+7evcvZ2RndbpdQKMTu7i57e3tsbW1RKBRIp9NXljyfebHNZpN6vc6jR4+4f/8+pmkyHo8Jh8Ps7u7yxRdfYBgGhULhysIAzLhYr3S90WjQ6/WYTCbA8zpeRVFIp9OkUini8fiVhQCPmRZr2zbFYpEHDx5QLBaxLAtJklhdXSWdTrO1tcXGxgaSJAV+3fp3Zlqs67oMBgO63S6DwQDXdRFF0Z9laZrm53KvelPKTIr10oDD4ZBiscjh4SG1Ws1PvNy6dYvNzU22trb8TNpVM7NivVRjs9mkXC7TbrcBUFWVzc1Ndnd3yeVy17bKO5Ni+/0+T58+pdFocHJywtnZGYIgsLi4yNLSEgsLC+RyOWKx2LW1cSbFtlot7t+/T7lc5vDwkJOTE1ZWVtjc3GR9fZ0bN27w3nvvXWtNwkyJtSzLzwl4tQrn5+fA8xDg9VRvwLpOZkas67rUajVOTk54+PAh3333HdVqlV6vh6qqbGxs8Nlnn7GwsEAqlbru5s6OWADTNGk0GlSrVU5OTqjVaiiK4iey8/m8n2+9bqZerOu6WJbFeDzm8PCQO3fuUCwW/ezV4uIi6XSa9fV1VldXSSQSl1pjC4qZEDsajRgOh/z+++/88MMP9Ho9X6xhGKytrbG2tsbKygqqql53k4EZEHt+fs7jx49pNBo8e/YM0zSxbdsvRSoUCmxvb7OwsDBVlYlTLdZLC37zzTc8ePCAJ0+eUK/XkSSJVCpFMpnk1q1bfPLJJxiGgSRJ191kn+n5iP+G4zhMJhNGoxH1ep1SqUSn08GyLEKhEJqmkUqlSKfTZDKZwAswLsvU9tjRaESn0+H09JRSqcTx8THD4RAAwzD49NNPWV5e5oMPPiCbzV5bTuBlTK1Yb9W11+vRbrf9XAA8z7fu7OywurrK4uLitU5dX8bUiR0Oh9i2zdHREXfv3qVUKtFoNBAEgUKhwPLyMjs7O2xsbEytVJgysV5+1TRNDg4O+Oqrr2g0Gn5KcHt7m9u3b7O2tsbNmzf9qsJpZKrEOo5Dv9/n7OyMZrNJp9NhMBjgOA7wPB+QTqfRdf1aVgUuw1S1bDKZ8PjxYx4+fMivv/5KvV5nMBgwHo8RRRHDMNje3sYwjKmYXf0TUyPWWxVotVpUKhWazSbD4RDLsvxjvEK5aDQ6VZOBFzEVYofDIY1Gg3a7zcHBAffu3eP09NRfdZ1FpkKsV9Pq1QccHBwwHo8Zj8fX3bTXZirEeuWXrVbL3zHjDVjeLhjv5lVmT9Nk4EVMhdjz83OOj48plUo0m01Go5FfDh8Oh0kmk0SjUX+raNBl7m+DqRDruq7/1fd2t3jll4qiYBiGX83t7XiZi31NYrEYmUyGhYUFvvzySzY3N7lx4wZLS0t+OJhmpk6stxdMlmUSiQS5XI79/X0++ugjVFW90sK2N2EqxMbjcXZ3d1ldXUXXdT7++GNUVSWZTJJKpcjn81Ox8noZhIv8c0e320XXdTqdTiC/xulVtriu62+o8/bFerF2GiYEl/FwoR7rue92u2/euhnGO/+L/IvMhcT2ej0ACoXCGzTr3aHX671yv9mFQsH8N7qfc5nf6L6Q2DmX5/pHhHeUudiAmIsNiLnYgJiLDYi52ICYiw2I/wBqTrmz4NeViAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "# Make predictions on new data\n",
        "predictions = model.predict(test_images)\n",
        "\n",
        "# Visualize the predictions\n",
        "class_names = [str(i) for i in range(5)]\n",
        "plt.figure(figsize=(5,5))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test_images[i], cmap=plt.cm.binary)\n",
        "    predicted_label = np.argmax(predictions[i])\n",
        "    true_label = test_labels[i]\n",
        "    if predicted_label == true_label:\n",
        "        color = 'green'\n",
        "    else:\n",
        "        color = 'red'\n",
        "    plt.xlabel(\"{} ({})\".format(class_names[predicted_label], \n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DO: Epoch is how many times the model is trained on the training data set. Change the number of epochs that the model is trained on and notice how that affects the loss and accuracy of the model.[link text]"
      ],
      "metadata": {
        "id": "QqMoSuQ_v-QC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The less epochs, the less accurate the prediction is, and vice versa."
      ],
      "metadata": {
        "id": "V1EMUY9XweYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DO: Change other parameters in the code block above and notice what changes your code made."
      ],
      "metadata": {
        "id": "irIKPrt-wemh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I changed the range from 10 to 5 and it only outputted one number and did not guess which number it was."
      ],
      "metadata": {
        "id": "0eRQLccowwLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your model: Use software to create a 28 x 28 image with a number drawn on it. Upload the image into the google colab workspace, change the path to the image and then run the model to see how well it identifies your 'handwriting'"
      ],
      "metadata": {
        "id": "rtHpzZLDwx1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load the trained model\n",
        "\n",
        "# Load the image and convert it to grayscale\n",
        "image = cv2.imread(\"1.jpg\")\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Resize the image to 28x28 pixels (the size expected by the model)\n",
        "resized = cv2.resize(gray, (28, 28), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "# Normalize the pixel values to be between 0 and 1\n",
        "normalized = resized / 255.0\n",
        "\n",
        "# Reshape the image to be a 1x28x28x1 array (the shape expected by the model)\n",
        "reshaped = normalized.reshape((1, 28, 28, 1))\n",
        "\n",
        "# Use the model to predict the label of the image\n",
        "prediction = model.predict(reshaped)[0]\n",
        "predicted_label = np.argmax(prediction)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create an output image with the predicted label\n",
        "output_image = np.zeros((100, 500, 3), dtype=np.uint8)\n",
        "cv2.putText(output_image, \"Prediction: {}\".format(predicted_label), (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2)\n",
        "\n",
        "# Resize the original image to have the same height as the output image\n",
        "image = cv2.resize(image, (int(output_image.shape[1] / output_image.shape[0] * image.shape[0]), output_image.shape[0]))\n",
        "\n",
        "# Display the image and the predicted label side by side\n",
        "cv2_imshow(np.hstack([image, output_image]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "tdVeapx_wvip",
        "outputId": "14b9d8b9-4122-4828-e686-c57a66c52ae6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=640x100 at 0x7FB2E1B43B80>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DO:Explore other common machine learning datasets\n",
        "\n",
        "Below are common machine learning datasets that you should be able to find resources on. See if you can find code to build a model and test the model using one of the datasets below.\n",
        "\n",
        "* CIFAR-10: Image classification dataset with 50,000 training images and 10,000 test images of size 32x32. Contains 10 classes of objects.\n",
        "\n",
        "*CIFAR-100: Image classification dataset with 50,000 training images and 10,000 test images of size 32x32. Contains 100 fine-grained classes of objects.\n",
        "\n",
        "* Fashion-MNIST: Clothing image classification dataset with 60,000 training images and 10,000 test images of size 28x28. Contains 10 classes of clothing items.\n",
        "\n",
        "*IMDB movie reviews: A binary sentiment analysis dataset of 25,000 movie reviews for training and 25,000 for testing. The reviews have been preprocessed and each one is associated with a label indicating a positive or negative sentiment."
      ],
      "metadata": {
        "id": "2JUE3Vqeye5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please don't make me try any more of these models they are not working and I just want it to recognize my 6 and 1 :("
      ],
      "metadata": {
        "id": "68xvOhMr8uLe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbhcY5q3zNKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}